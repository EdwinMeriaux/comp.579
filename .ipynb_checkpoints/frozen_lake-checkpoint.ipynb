{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5395aaf6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2126b274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065f67f9",
   "metadata": {},
   "source": [
    "# Basic test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bff4c4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state of the system\n",
      "Iteration: 1 and action 0\n",
      "Iteration: 2 and action 0\n",
      "Iteration: 3 and action 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20232\\3503324804.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Iteration: {} and action {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandomAction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturnValue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#basic test\n",
    "\n",
    "\n",
    "env=gym.make(\"FrozenLake-v1\")#,render_mode='human')\n",
    "env.reset()\n",
    "env.render()\n",
    "print('Initial state of the system')\n",
    " \n",
    "numberOfIterations=30\n",
    " \n",
    "for i in range(numberOfIterations):\n",
    "    randomAction= env.action_space.sample()\n",
    "    returnValue=env.step(randomAction)\n",
    "    env.render()\n",
    "    print('Iteration: {} and action {}'.format(i+1,randomAction))\n",
    "    time.sleep(2)\n",
    "    if returnValue[2]:\n",
    "        break\n",
    " \n",
    "env.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d20f1ea",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9892cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_pos(val,size):\n",
    "    x = int(val/size)\n",
    "    y = val % size\n",
    "    return[x,y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f2a5ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns max value of list\n",
    "#if multiple are equal then randomization occurs between those possibilities\n",
    "def max_return(possiblities):\n",
    "    top = [0]\n",
    "    current_return = 0\n",
    "\n",
    "    pos = 0\n",
    "    for i in possiblities:\n",
    "        if i > current_return:\n",
    "            top = [pos]\n",
    "            current_return = i\n",
    "        elif i == current_return:\n",
    "            top.append(pos)\n",
    "        \n",
    "        pos += 1\n",
    "    \n",
    "    if len(top) > 0:\n",
    "        random_val = np.random.randint(0,len(top),1)\n",
    "        return(top[random_val[0]])\n",
    "    else:\n",
    "        return(top[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we start with greedy then we will go to sarsa\n",
    "def E_greedy_action(epsilon, world, current):\n",
    "    random_action = np.random.randint(0,100,2)\n",
    "    if random_action[0] < epsilon:\n",
    "        action = np.random.randint(0,100,1)\n",
    "    else:\n",
    "        action = 0 #max return\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f4502a",
   "metadata": {},
   "source": [
    "# SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522eee0c",
   "metadata": {},
   "source": [
    "Things to do:\n",
    "1. sarsa from greedy (conditionally done)\n",
    "2. softmax from epsilon\n",
    "3. updating rewards (conditionally done)\n",
    "4. run traning (run full)\n",
    "5. egreedy not working\n",
    "6. expected sarsa\n",
    "7. graphs\n",
    "8. modify sarsa reward to take reward based on the location arrived and not where it should land (conditionally done)\n",
    "9. what is temperature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4ea6656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA:\n",
    "    def __init__(self,size,epsilon,explore):\n",
    "        self.runs = 10\n",
    "        self.segments = 500\n",
    "        self.train_episodes = 10\n",
    "        self.test_episodes = 1\n",
    "        self.world_size = size\n",
    "        self.world = np.zeros((self.world_size,self.world_size,4))\n",
    "        self.world_usage = np.zeros((self.world_size,self.world_size,4))\n",
    "        self.current = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.action = 0\n",
    "        \n",
    "        #environment initialization\n",
    "        self.env=gym.make(\"FrozenLake-v1\", is_slippery=False)#,render_mode='human')\n",
    "        self.env.reset()\n",
    "        self.env.render()\n",
    "        \n",
    "        #path\n",
    "        self.current_path = []\n",
    "\n",
    "        #exploration number\n",
    "        self.exploration = explore\n",
    "        self.decay = epsilon/explore\n",
    "        self.alpha = 0.1\n",
    "        self.gamma = 0.1\n",
    "        self.reward = 0\n",
    "        self.tmp_next = 0\n",
    "        \n",
    "        self.temperature = 1\n",
    "    \n",
    "    def modified_start(self):\n",
    "        self.action = 1\n",
    "        returnValue=self.env.step(self.action) #take next step\n",
    "        self.env.render()\n",
    "        \n",
    "        self.action = 1\n",
    "        returnValue=self.env.step(self.action) #take next step\n",
    "        self.env.render()\n",
    "        \n",
    "        self.action = 2\n",
    "        returnValue=self.env.step(self.action) #take next step\n",
    "        self.env.render()\n",
    "        \n",
    "        self.action = 1\n",
    "        returnValue=self.env.step(self.action) #take next step\n",
    "        self.env.render()\n",
    "        \n",
    "        #self.action = 2\n",
    "        #returnValue=self.env.step(self.action) #take next step\n",
    "        #self.env.render()\n",
    "        \n",
    "        self.current = returnValue[0]\n",
    "    def modified_start1(self):\n",
    "        self.action = 1\n",
    "        returnValue=self.env.step(self.action) #take next step\n",
    "        self.env.render()\n",
    "        \n",
    "        self.action = 1\n",
    "        returnValue=self.env.step(self.action) #take next step\n",
    "        self.env.render()\n",
    "        \n",
    "        self.action = 2\n",
    "        returnValue=self.env.step(self.action) #take next step\n",
    "        self.env.render()\n",
    "        \n",
    "        self.action = 1\n",
    "        returnValue=self.env.step(self.action) #take next step\n",
    "        self.env.render()\n",
    "        \n",
    "        self.action = 2\n",
    "        returnValue=self.env.step(self.action) #take next step\n",
    "        self.env.render()\n",
    "        \n",
    "        self.current = returnValue[0]\n",
    "    def grid_pos(self,val):\n",
    "        x = int(val/self.world_size)\n",
    "        y = val % self.world_size\n",
    "        return[x,y]\n",
    "    \n",
    "    def max_return(self,possiblities):\n",
    "        top = []\n",
    "        print(\"possibilities: \",possiblities)\n",
    "        current_return = 0\n",
    "        pos = 0\n",
    "        for i in possiblities:\n",
    "            if i > current_return:\n",
    "                top = [pos]\n",
    "                current_return = i\n",
    "            elif i == current_return:\n",
    "                top.append(pos)\n",
    "            pos += 1\n",
    "        #print(top)\n",
    "        if len(top) > 0:\n",
    "            random_val = np.random.randint(0,len(top),1)\n",
    "            return(top[random_val[0]])\n",
    "        else:\n",
    "            return(top[0])\n",
    "    \n",
    "    def max_reward_return(self,possiblities):\n",
    "        top = []\n",
    "        #print(\"possibilities: \",possiblities)\n",
    "        current_return = 0\n",
    "        pos = 0\n",
    "        for i in possiblities:\n",
    "            if i > current_return:\n",
    "                top = [pos]\n",
    "                current_return = i\n",
    "            elif i == current_return:\n",
    "                top.append(pos)\n",
    "            pos += 1\n",
    "        #print(top)\n",
    "        return current_return\n",
    "    \n",
    "    def next_space_max_reward(self,movement):\n",
    "        reward = 0\n",
    "        if movement == 3:\n",
    "            pos = self.grid_pos(self.current)\n",
    "            if pos[0] - 1 < 0:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = self.max_reward_return(self.world[pos[0]-1][pos[1]]) #returns action not state\n",
    "        \n",
    "        elif movement == 1:\n",
    "            pos = self.grid_pos(self.current)\n",
    "            if pos[0] + 1 > 3:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = self.max_reward_return(self.world[pos[0]+1][pos[1]]) #returns action not state\n",
    "        \n",
    "        elif movement == 0:\n",
    "            pos = self.grid_pos(self.current)\n",
    "            if pos[1] - 1 < 0:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = self.max_reward_return(self.world[pos[0]][pos[1]-1]) #returns action not state\n",
    "        \n",
    "        elif movement == 2:\n",
    "            pos = self.grid_pos(self.current)\n",
    "            if pos[1] + 1 > 3:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = self.max_reward_return(self.world[pos[0]][pos[1]+1]) #returns action not state\n",
    "        #print(\"reward second stage: \",reward)\n",
    "        return reward\n",
    "    def next_space_max_reward_flip(self,movement):\n",
    "        reward = 0\n",
    "        if movement == 0:\n",
    "            pos = self.grid_pos(self.current)\n",
    "            if pos[1] - 1 < 0:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = self.max_reward_return(self.world[pos[1]-1][pos[0]]) #returns action not state\n",
    "        \n",
    "        elif movement == 2:\n",
    "            pos = self.grid_pos(self.current)\n",
    "            if pos[1] + 1 > 3:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = self.max_reward_return(self.world[pos[1]+1][pos[0]]) #returns action not state\n",
    "        \n",
    "        elif movement == 3:\n",
    "            pos = self.grid_pos(self.current)\n",
    "            if pos[0] - 1 < 0:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = self.max_reward_return(self.world[pos[1]][pos[0]-1]) #returns action not state\n",
    "        \n",
    "        elif movement == 1:\n",
    "            pos = self.grid_pos(self.current)\n",
    "            if pos[0] + 1 > 3:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = self.max_reward_return(self.world[pos[1]][pos[0]+1]) #returns action not state\n",
    "        #print(\"reward second stage: \",reward)\n",
    "        return reward\n",
    "    def next_space_max_reward_give_max(self,movement):\n",
    "        reward = 0\n",
    "        max_reward = 0\n",
    "        \n",
    "        pos = self.grid_pos(self.current)\n",
    "        #print(\"position for future max: \",pos)\n",
    "        if pos[1] - 1 < 0:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = self.max_reward_return(self.world[pos[1]-1][pos[0]]) #returns action not state\n",
    "            #print(\"reward 1: \",reward)\n",
    "            if reward > max_reward:\n",
    "                max_reward = reward\n",
    "\n",
    "\n",
    "        if pos[1] + 1 > 3:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = self.max_reward_return(self.world[pos[1]+1][pos[0]]) #returns action not state\n",
    "            #print(\"reward 2: \",reward)\n",
    "            if reward > max_reward:\n",
    "                max_reward = reward\n",
    "        \n",
    "        if pos[0] - 1 < 0:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = self.max_reward_return(self.world[pos[1]][pos[0]-1]) #returns action not state\n",
    "            #print(\"reward 3: \",reward)\n",
    "            if reward > max_reward:\n",
    "                max_reward = reward\n",
    "        \n",
    "        if pos[0] + 1 > 3:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = self.max_reward_return(self.world[pos[1]][pos[0]+1]) #returns action not state\n",
    "            #print(\"reward 4: \",reward)\n",
    "            if reward > max_reward:\n",
    "                max_reward = reward\n",
    "                \n",
    "        #print(\"reward second stage: \",reward)\n",
    "        return max_reward\n",
    "    \n",
    "    def next_space_max_reward_give_max_nonflip(self,movement):\n",
    "        reward = 0\n",
    "        max_reward = 0\n",
    "        \n",
    "        pos = self.grid_pos(self.current)\n",
    "        #print(\"position for future max: \",pos)\n",
    "        if pos[0] - 1 < 0:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = self.max_reward_return(self.world[pos[0]-1][pos[1]]) #returns action not state\n",
    "            #print(\"reward 1: \",reward)\n",
    "            if reward > max_reward:\n",
    "                max_reward = reward\n",
    "\n",
    "\n",
    "        if pos[0] + 1 > 3:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = self.max_reward_return(self.world[pos[0]+1][pos[1]]) #returns action not state\n",
    "            #print(\"reward 2: \",reward)\n",
    "            if reward > max_reward:\n",
    "                max_reward = reward\n",
    "        \n",
    "        if pos[1] - 1 < 0:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = self.max_reward_return(self.world[pos[0]][pos[1]-1]) #returns action not state\n",
    "            #print(\"reward 3: \",reward)\n",
    "            if reward > max_reward:\n",
    "                max_reward = reward\n",
    "        \n",
    "        if pos[1] + 1 > 3:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = self.max_reward_return(self.world[pos[0]][pos[1]+1]) #returns action not state\n",
    "            #print(\"reward 4: \",reward)\n",
    "            if reward > max_reward:\n",
    "                max_reward = reward\n",
    "                \n",
    "        #print(\"reward second stage: \",reward)\n",
    "        return max_reward\n",
    "        \n",
    "    def future_max_return(self,possiblities): #includes future return (TBD)\n",
    "        top = []\n",
    "        #print(\"max: \",possiblities)\n",
    "        current_return = 0\n",
    "        pos = 0\n",
    "        for i in possiblities:\n",
    "            #print(\"i\",i,pos)\n",
    "            #new_return = i + self.next_space_max_reward_flip(pos)\n",
    "            new_return = i\n",
    "            if new_return > current_return:\n",
    "            #if i > current_return:\n",
    "                top = [pos]\n",
    "                current_return = new_return\n",
    "            elif new_return == current_return:\n",
    "                top.append(pos)\n",
    "            pos += 1\n",
    "        #print(top)\n",
    "        if len(top) > 0:\n",
    "            random_val = np.random.randint(0,len(top),1)\n",
    "            return(top[random_val[0]])\n",
    "        else:\n",
    "            return(top[0])\n",
    "    \n",
    "    def E_greedy_action(self):\n",
    "        random_action = np.random.randint(0,100,2)\n",
    "        if False: #random_action[0] < self.epsilon:\n",
    "            action = np.random.randint(0,4,1)[0]\n",
    "            print(\"random: \",action)\n",
    "        else:\n",
    "            pos = self.grid_pos(self.current)\n",
    "            print(\"current position: \",pos)\n",
    "            action = self.max_return(self.world[pos[0]][pos[1]])\n",
    "            print(\"non random: \", action)\n",
    "        #print(\"action: \",action)\n",
    "        return action\n",
    "    \n",
    "    def E_greedy_next_step_action(self):\n",
    "        random_action = np.random.randint(0,100,2)\n",
    "        if False: #random_action[0] < self.epsilon:\n",
    "            action = np.random.randint(0,4,1)[0]\n",
    "            print(\"random: \",action)\n",
    "        else:\n",
    "            pos = self.grid_pos(self.tmp_next)\n",
    "            print(\"current position: \",pos)\n",
    "            action = self.max_return(self.world[pos[0]][pos[1]])\n",
    "            print(\"non random: \", action)\n",
    "        #print(\"action: \",action)\n",
    "        return action\n",
    "    \n",
    "    def softmax(self):\n",
    "        pos = self.grid_pos(self.current)\n",
    "        posible_action = self.world[pos[0]][pos[1]]\n",
    "        probabilities = np.exp(posible_action/self.temperature)/np.sum(np.exp(posible_action)/self.temperature)     \n",
    "        x = random.choice(probabilities)\n",
    "        action = probabilities.index(x)\n",
    "        return action\n",
    "    \n",
    "    def SARSA(self): #currently greed based\n",
    "        action = [0,0]\n",
    "        random_action = np.random.randint(0,100,2)\n",
    "        if random_action[0] < self.epsilon:\n",
    "            action = np.random.randint(0,4,1)[0]\n",
    "            #print(\"rand\")\n",
    "        else:\n",
    "            #print(\"max\")\n",
    "            pos = self.grid_pos(self.current)\n",
    "            action = self.future_max_return(self.world[pos[0]][pos[1]])\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def SARSA_reward(self):\n",
    "        quality = 0\n",
    "        pos = self.grid_pos(self.current)\n",
    "        print(\"current pos: \",self.current)\n",
    "        s_a_reward = self.world[pos[0]][pos[1]][self.action]\n",
    "        print(\"current reward: \",s_a_reward)\n",
    "        print(\"self.reward: \",self.reward)\n",
    "        #self.world_usage[pos[0]][pos[1]][self.action] = s_a_reward + self.alpha*(self.reward - s_a_reward + self.gamma * next_space_max_reward(self.action))\n",
    "        next_reward = self.next_space_max_reward(self.action)\n",
    "        print(\"next_reward: \",next_reward)\n",
    "        quality = s_a_reward + self.alpha*(self.reward - s_a_reward + self.gamma * next_reward)\n",
    "        print(\"quality update: \",quality)\n",
    "        \n",
    "        return quality\n",
    "    \n",
    "    def new_SARSA_reward(self):\n",
    "        quality = 0\n",
    "        #estimated reward for this step is based on the estimation from the original position\n",
    "        pos = self.grid_pos(self.current)\n",
    "        print(\"current pos: \",self.current)\n",
    "        s_a_reward = self.world[pos[0]][pos[1]][self.action]\n",
    "        \n",
    "        #next reward is based on the location arrived\n",
    "        print(\"current reward: \",s_a_reward)\n",
    "        print(\"self.reward: \",self.reward)\n",
    "        next_action = self.E_greedy_next_step_action()\n",
    "        pos = self.grid_pos(self.tmp_next)\n",
    "        next_reward = self.world[pos[0]][pos[1]][next_action]\n",
    "        print(\"next_reward: \",next_reward)\n",
    "        \n",
    "        quality = s_a_reward + self.alpha*(self.reward - s_a_reward + self.gamma * next_reward)\n",
    "        print(\"quality update: \",quality)\n",
    "        \n",
    "        return quality\n",
    "    \n",
    "    def loop(self):\n",
    "        for j in range(500):\n",
    "            for i in range(10):\n",
    "                self.current = 0\n",
    "                self.current_path = [self.current]\n",
    "\n",
    "                self.action = 0\n",
    "                returnValue=self.env.step(self.action)\n",
    "\n",
    "                print('Initial state of the system')\n",
    "                done = False\n",
    "                self.env.reset()\n",
    "                print(\"+++++++++++++++\",i,j,\"+++++++++++\")\n",
    "                while not done:\n",
    "\n",
    "                    self.action = self.softmax() #greedy based action\n",
    "                    print(self.action)\n",
    "\n",
    "                    returnValue=self.env.step(self.action) #take next step\n",
    "                    self.reward=returnValue[1] #get reward\n",
    "                    self.tmp_next=returnValue[0] #get position\n",
    "                    print(returnValue)\n",
    "\n",
    "                    pos = self.grid_pos(self.current)\n",
    "                    print(\"current position: \",pos)\n",
    "                    self.world[pos[0]][pos[1]][self.action] = self.new_SARSA_reward() #update world reward\n",
    "                    print(\"world state: \",self.world)\n",
    "\n",
    "                    self.world_usage[pos[0]][pos[1]][self.action] += 1 #update state action usage count\n",
    "\n",
    "                    self.current = returnValue[0] #update current location\n",
    "                    self.current_path.append(self.current) #update path taken\n",
    "                    done = returnValue[2] #update terminal check\n",
    "                    self.env.render()#render new state\n",
    "                    time.sleep(1)\n",
    "                    print(\"--------------------------------\")\n",
    "                print(done)\n",
    "                reward = returnValue[1]\n",
    "                if reward > 0:\n",
    "                    pos = self.grid_pos(self.current)\n",
    "                    self.world[pos[0]][pos[1]]\n",
    "\n",
    "                time.sleep(5)\n",
    "\n",
    "                print(self.current_path)\n",
    "\n",
    "                self.exploration -= self.decay\n",
    "                if self.exploration < 0:\n",
    "                    self.exploration = 0\n",
    "            \n",
    "            for i in range(1):\n",
    "                self.current = 0\n",
    "                self.current_path = [self.current]\n",
    "\n",
    "                self.action = 0\n",
    "                returnValue=self.env.step(self.action)\n",
    "\n",
    "                print('Initial state of the system')\n",
    "                done = False\n",
    "                self.env.reset()\n",
    "                print(\"+++++++++++++++ test\",j,\"+++++++++++\")\n",
    "                while not done:\n",
    "\n",
    "                    self.action = self.E_greedy_action() #greedy based action\n",
    "                    print(self.action)\n",
    "\n",
    "                    returnValue=self.env.step(self.action) #take next step\n",
    "                    self.reward=returnValue[1] #get reward\n",
    "                    self.tmp_next=returnValue[0] #get position\n",
    "                    print(returnValue)\n",
    "\n",
    "                    pos = self.grid_pos(self.current)\n",
    "                    print(\"current position: \",pos)\n",
    "                    self.world[pos[0]][pos[1]][self.action] = self.new_SARSA_reward() #update world reward\n",
    "                    print(\"world state: \",self.world)\n",
    "\n",
    "                    self.world_usage[pos[0]][pos[1]][self.action] += 1 #update state action usage count\n",
    "\n",
    "                    self.current = returnValue[0] #update current location\n",
    "                    self.current_path.append(self.current) #update path taken\n",
    "                    done = returnValue[2] #update terminal check\n",
    "                    self.env.render()#render new state\n",
    "                    time.sleep(1)\n",
    "                    print(\"--------------------------------\")\n",
    "                print(done)\n",
    "                reward = returnValue[1]\n",
    "                if reward > 0:\n",
    "                    pos = self.grid_pos(self.current)\n",
    "                    self.world[pos[0]][pos[1]]\n",
    "\n",
    "                time.sleep(5)\n",
    "\n",
    "                print(self.current_path)\n",
    "\n",
    "                self.exploration -= self.decay\n",
    "                if self.exploration < 0:\n",
    "                    self.exploration = 0\n",
    "            \n",
    "        self.env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4e8b5609",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa = SARSA(4,0,200)\n",
    "#sarsa.loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a435bf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25 0.25 0.25 0.25]\n"
     ]
    }
   ],
   "source": [
    "print(sarsa.softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56ccb95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26921435 0.24359522 0.24359522 0.24359522]\n",
      "[0.35339549 0.23688808 0.23688808 0.23688808]\n"
     ]
    }
   ],
   "source": [
    "sarsa.world[0][0][0] = .1\n",
    "print(sarsa.softmax())\n",
    "sarsa.temperature = .5\n",
    "sarsa.world[0][0][0] = .2\n",
    "print(sarsa.softmax()/.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a8085d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66216045 0.24359522 0.24359522 0.24359522]\n",
      "[0.29752787 0.24359522 0.24359522 0.24359522]\n",
      "[0.26921435 0.24359522 0.24359522 0.24359522]\n",
      "[0.24604339 0.24359522 0.24359522 0.24359522]\n"
     ]
    }
   ],
   "source": [
    "sarsa.temperature = .1\n",
    "sarsa.world[0][0][0] = .1\n",
    "print(sarsa.softmax()/sarsa.temperature)\n",
    "sarsa.temperature = .5\n",
    "sarsa.world[0][0][0] = .1\n",
    "print(sarsa.softmax()/sarsa.temperature)\n",
    "sarsa.temperature = 1\n",
    "sarsa.world[0][0][0] = .1\n",
    "print(sarsa.softmax()/sarsa.temperature)\n",
    "sarsa.temperature = 10\n",
    "sarsa.world[0][0][0] = .1\n",
    "print(sarsa.softmax()/sarsa.temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5f14b5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00000001\n",
      "0.97682905\n"
     ]
    }
   ],
   "source": [
    "print(0.26921435 + 0.24359522 + 0.24359522 + 0.24359522)\n",
    "print(0.24604339 + 0.24359522 + 0.24359522 + 0.24359522)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "9807b0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.34390000000000004\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sarsa.next_space_max_reward(0))\n",
    "print(sarsa.next_space_max_reward(1))\n",
    "print(sarsa.next_space_max_reward(2))\n",
    "print(sarsa.next_space_max_reward(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "d096dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa.current = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "1d411601",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa = SARSA(4,0,10)\n",
    "sarsa.world[0][0][1] = 5\n",
    "sarsa.world[0][0][2] = 10\n",
    "sarsa.world[0][1][1] = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "ec549adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  5. 10.  0.]\n",
      "  [ 0. 15.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(sarsa.world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "6ad6049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non random:  2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(sarsa.E_greedy_action())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "5abacfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possibilities:  [0. 0. 0. 0.]\n",
      "[0, 1, 2, 3]\n",
      "reward second stage:  0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sarsa.next_space_max_reward_flip(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "68ace79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possibilities:  [ 0.  5. 10.  0.]\n",
      "[2]\n",
      "action:  2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(sarsa.E_greedy_action())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b19ce88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possibilities:  [0. 0. 0. 0.]\n",
      "[0, 1, 2, 3]\n",
      "reward second stage:  0\n",
      "quality update:  0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "sarsa.action = 2\n",
    "print(sarsa.SARSA_reward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "69bf5a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa.env.step(2)\n",
    "sarsa.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e4cead04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beedf6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 10\n",
    "segments = 500\n",
    "train_episodes = 10\n",
    "test_episodes = 1\n",
    "world_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "06636c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "world = np.zeros((4,4,4))\n",
    "print(world[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8410ac59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c15e2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f45166e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n",
      "30\n",
      "30\n",
      "10\n",
      "10\n",
      "20\n",
      "10\n",
      "10\n",
      "30\n",
      "30\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "30\n",
      "30\n",
      "20\n",
      "10\n",
      "20\n",
      "30\n",
      "10\n",
      "10\n",
      "20\n",
      "20\n",
      "30\n",
      "20\n",
      "10\n",
      "20\n",
      "20\n",
      "10\n",
      "10\n",
      "20\n",
      "30\n",
      "10\n",
      "10\n",
      "10\n",
      "20\n",
      "20\n",
      "10\n",
      "10\n",
      "20\n",
      "10\n",
      "10\n",
      "10\n",
      "30\n",
      "30\n",
      "20\n",
      "10\n",
      "30\n",
      "30\n",
      "10\n",
      "30\n",
      "20\n",
      "30\n",
      "10\n",
      "20\n",
      "30\n",
      "20\n",
      "30\n",
      "20\n",
      "20\n",
      "10\n",
      "30\n",
      "30\n",
      "20\n",
      "20\n",
      "20\n",
      "30\n",
      "30\n",
      "10\n",
      "30\n",
      "20\n",
      "20\n",
      "20\n",
      "10\n",
      "10\n",
      "30\n",
      "10\n",
      "20\n",
      "20\n",
      "10\n",
      "30\n",
      "30\n",
      "20\n",
      "10\n",
      "30\n",
      "20\n",
      "30\n",
      "10\n",
      "10\n",
      "30\n",
      "30\n",
      "20\n",
      "30\n",
      "10\n",
      "30\n",
      "30\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(100):\n",
    "    sampleList = [10, 20, 30]\n",
    "    x = random.choice(sampleList)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b6d24f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
